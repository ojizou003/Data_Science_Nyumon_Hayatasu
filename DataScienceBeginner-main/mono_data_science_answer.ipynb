{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データサイエンス入門\n",
    "\n",
    "このノートブックは、YouTubeで紹介している以下の講義用です。\n",
    "\n",
    "https://www.youtube.com/playlist?list=PL4Y-mUWLK2t0Vy2sUIXK3ItMX0s7CvoB_\n",
    "\n",
    "これ単体でも学習できますが、動画の解説を前提に作成している部分があるので、ぜひ動画を合わせて見ていただけると嬉しいです。\n",
    "\n",
    "## (復習)データサイエンスの7ステップ\n",
    "\n",
    "データサイエンスには、以下7つのステップがありました。\n",
    "\n",
    "1. 目的・課題の特定\n",
    "2. データの取得・収集\n",
    "3. データ理解・データの可視化\n",
    "4. データの加工・前処理\n",
    "5. 機械学習モデルの作成\n",
    "6. 評価・テスト\n",
    "7. レポーティング or アプリケーション化\n",
    "\n",
    "この動画シリーズでは、「2. データの取得・収集」から「6. 評価・テスト」までを体験していきます。\n",
    "\n",
    "基本的に、各セクションごとに動画を区切っていきます。なので途中で休憩しながら学習したり、必要な部分だけピンポイントで復習に使っていただけると良いかと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの取得・収集\n",
    "\n",
    "今回使っていくデータは、Kaggleからダウンロードした**タイタニック号のデータ**です。\n",
    "\n",
    "https://www.kaggle.com/c/titanic\n",
    "\n",
    "あとはダウンロードしたデータをPythonで使えるようにすれば、さっそくデータ分析を開始できます。\n",
    "\n",
    "<br>\n",
    "\n",
    "今回の動画シリーズでは、**Google Colaboratory**を使って学習を進めていきます。\n",
    "\n",
    "*※ネット回線に繋ぐのが面倒であれば、お手元のPCに入っているAnacondaでも学習できます。その場合は、ファイルパスを変更するだけで同じように学習できるようになっています！*\n",
    "\n",
    "<br>\n",
    "\n",
    "Google Colabでデータを読み込むためには、まず**Google Driveをマウントする必要があります。**\n",
    "\n",
    "要するにGoogle Driveを使えるようにしないといけないってことです！何も設定しなければ、Google Driveに入っているデータを読み込むことはできません。\n",
    "\n",
    "<br>\n",
    "\n",
    "というわけで、さっそくGoogle Driveのマウントからやっていきましょう。\n",
    "\n",
    "Google Driveをマウントするには、以下のコードを実行します。コードを実行すると、URLと入力ボックスが出現するはずです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Driveの認証を済ませるには、上記のURLにアクセスして認証コードをコピーします。\n",
    "\n",
    "その後、認証コードを入力ボックスにペーストすれば、Google Driveのマウントが完了です。\n",
    "\n",
    "<br>\n",
    "\n",
    "これでデータサイエンスを体験する準備ができました。次回からPythonを使ってデータに触れていきましょう！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# データ理解・データの可視化\n",
    "\n",
    "まずはじめに、タイタニック号のデータを理解することから始めます。\n",
    "\n",
    "- どんなデータが入っているんだろう？\n",
    "- このデータには、どんな特徴があるんだろう？\n",
    "- 欠けているデータはあるのかな？\n",
    "\n",
    "上記のような内容を、このステップで見ていくことになります。\n",
    "\n",
    "<br>\n",
    "\n",
    "データサイエンスと聞くと、とりあえず「*機械学習を使って色々やっていくんですよね〜？*」と思われるかもしれません。\n",
    "\n",
    "でも、その機械学習を使うためにも、まずはデータを理解しないといけません。\n",
    "\n",
    "内見をせず家を買ったりしませんよね？それと同じです。まずは、中身を知ることから始めていきます。\n",
    "\n",
    "<br>\n",
    "\n",
    "というわけで、まずはライブラリのインポートと、ファイルの読み込みから始めていきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリのインポート\n",
    "\n",
    "今回使うライブラリをインポートします。\n",
    "\n",
    "*※Pythonをはじめて学習する場合は、ライブラリのインポート = Pythonを便利にするためのおまじないだと思ってください!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリにインポート\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は、上記4つのライブラリをインポートしました。\n",
    "\n",
    "これらのライブラリはデータ分析するとき、ほぼマストで必要になります。\n",
    "\n",
    "<br>\n",
    "\n",
    "各ライブラリの特徴は、以下のとおりです。\n",
    "\n",
    "- numpy : 行列計算や数値を扱いやすくするライブラリ。pandasと合わせて使うことが多いです。\n",
    "- pandas : データを扱いやすくするライブラリ。かなり重要です。\n",
    "- matplotlib : グラフを作成するためのライブラリ。\n",
    "- seaborn : グラフをキレイかつ簡潔に書くためのライブラリ。\n",
    "\n",
    "まさにデータ分析といった感じですよね。\n",
    "\n",
    "\n",
    "今回は、これらのライブラリで使う部分だけピックアップして紹介していきます。\n",
    "\n",
    "*※各ライブラリは、できることがたくさんあるので、それだけでかなりのボリュームになってしまいます...。需要があれば、各ライブラリの使い方を詳しく紹介する動画を作っていきたいと思うので、コメントにてお知らせしていただけると嬉しいです><*\n",
    "\n",
    "<br>\n",
    "\n",
    "それでは、これらのライブラリを使って「データ理解・データの可視化」をやっていきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込み\n",
    "\n",
    "まずは今回使うデータの読み込みからはじめていきます。\n",
    "\n",
    "Python単体を使ってデータの読み込みをすることもできますが、ここでは`Pandas`を使うのが一般的です。\n",
    "\n",
    "<br>\n",
    "\n",
    "`Pandas`を`pd`と省略して使えるようにしておいたので、以下のように書くことでデータの読み込みが完了します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local\n",
    "# dir_path = './titanic/'\n",
    "dir_path = '/content/drive/MyDrive/DataScience/titanic/' # google colab\n",
    "\n",
    "# 学習データの読み込み\n",
    "train_df = pd.read_csv(dir_path + 'train.csv')\n",
    "\n",
    "# テストデータの読み込み\n",
    "test_df = pd.read_csv(dir_path + 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これだけで今回使うCSVファイルを読み込めました。\n",
    "\n",
    "この`pd.read_csv`を使うことで、表形式でデータの読み込みができます。(=表形式のことを、データフレームと言います。)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## どんなデータが入っているんだろう？\n",
    "\n",
    "それでは、どんなデータが入っているのか確認していきたいと思います。\n",
    "\n",
    "読み込んだファイルの中身を確認していきたいのですが、それには`train_df.head()`のように書いてあげます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データの先頭5行を確認してみる\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_df.head()`を使うことで、読み込んだデータの先頭5行が表示されました。\n",
    "\n",
    "*※`df.head()`は、デフォルトで先頭5行を表示するように設定されています。*\n",
    "\n",
    "<br>\n",
    "\n",
    "ヘッダーを見ると、色々なデータが入っていることが分かります。\n",
    "\n",
    "なお、各ヘッダーに対応する縦列のことを**カラム(Column)**と言います。\n",
    "\n",
    "現場に出ると、各カラムが「どういうデータなのか」といった説明が書いてあるドキュメント(=項目定義書)が準備されていることが多いです。\n",
    "\n",
    "<br>\n",
    "\n",
    "今回データをダウンロードしたKaggleにも各カラムの説明が書いてありますが、すべて英語のテキストになっています。\n",
    "\n",
    "なので、以下に日本語のカラム説明を書いておきました。\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>項目</th>\n",
    "        <th>定義(各項目の説明)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>PassengerId</td>\n",
    "        <td>乗客のID(ユニーク)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Survived</td>\n",
    "        <td>生存フラグ（0=死亡、1=生存）</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Pclass</td>\n",
    "        <td>チケットのクラス（1が最も良いクラス）</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Name</td>\n",
    "        <td>乗客の名前</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Sex</td>\n",
    "        <td>性別（male=男性、female＝女性）</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Age</td>\n",
    "        <td>乗客の年齢</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>SibSp</td>\n",
    "        <td>同乗している兄弟/配偶者の数</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>parch</td>\n",
    "        <td>同乗している親/子供の数</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>ticket</td>\n",
    "        <td>チケット番号</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>fare</td>\n",
    "        <td>料金</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>cabin</td>\n",
    "        <td>客室番号</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Embarked</td>\n",
    "        <td>タイタニック号に乗った港</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "今回使うデータは、上記のような項目になっています。\n",
    "\n",
    "<br>\n",
    "\n",
    "学習データには、このテーブルに書かれているすべての項目が入っていますね。\n",
    "\n",
    "それでは、テストデータの中身ものぞいてみましょう。学習データと同様に`test_df.head()`を使うことで確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# テストデータの先頭5行を確認してみる\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そうすると、学習データと比較して、ヘッダーが少ないように感じます。\n",
    "\n",
    "実際に、各データの大きさを確認してみましょう。データの大きさを確認するには、`df.shape`を使ってあげます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフレームの大きさ\n",
    "print('学習データの大きさ:', train_df.shape)\n",
    "print('テストデータの大きさ:', test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そうすると(〇〇, 〇〇)のような形で出力されました。これは(行数, 列数)を意味しています。\n",
    "\n",
    "最初は行と列がややこしいかもしれません。行は縦の数で、列は横の数になります。\n",
    "\n",
    "なので今回でいうと、行数は乗客の数、列数はヘッダー情報の数です。\n",
    "\n",
    "<br>\n",
    "\n",
    "以上のことを踏まえると、テストデータは乗客の数も少ないですが、同時にヘッダー情報も1つ少なくなっています。\n",
    "\n",
    "学習データと比較して何が足りなくなっているのか確認してみると、**`Survived`が抜け落ちている**ことが分かります。\n",
    "\n",
    "<br>\n",
    "\n",
    "「こんなデータじゃ使い物にならないじゃないか...！」と思われるかもしれませんが、そんなことはありません。\n",
    "\n",
    "なぜなら`Survived`、つまり「生存しているか否か」が今回予測することだからです。\n",
    "\n",
    "つまり、**学習データ(=train_df)を使って、テストデータ(=test_df)の人たちが「生存するか否か」を当てるのが、今回予測したいこと**になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## このデータには、どんな特徴があるんだろう？\n",
    "\n",
    "どんなデータが入っているのか分かったところで、次にどのような特徴があるのか確認していきましょう。\n",
    "\n",
    "### データの連結\n",
    "\n",
    "その前に、学習データとテストデータを連結しておきましょう。データを見ていくとき、学習データとテストデータが分かれている必要がないですし、分けておくと同じコードを2回実行する必要が出てきてしまうので！\n",
    "\n",
    "*※今回はKaggleを使っているので、丁寧に学習データとテストデータに分割されていますが、通常はすべてまとまったデータになっています。*\n",
    "\n",
    "<br>\n",
    "\n",
    "データを縦に連結するときは、以下のように`pd.concat()`を使ってあげます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データとテストデータを連結する\n",
    "df = pd.concat([train_df, test_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この1行で、データの連結が完了しました。\n",
    "\n",
    "`ignore_index=True`としておくことで、連結するときのインデックス番号(=1番左の番号)を振り直しできます\n",
    "\n",
    "*※逆に「`ignore_index=True`を付けないとどうなるか？」は、ぜひ試してみてください！*\n",
    "\n",
    "<br>\n",
    "\n",
    "実際に、うまく連結できているか確認してみましょう。データの大きさを見るときは、`df.shape`でしたね！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 連結したデータの大きさを確認する\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そうすると、(1309, 12)と表示されました、学習データが891行、テストデータが418行だったのでしっかりと連結できていることが分かります。\n",
    "\n",
    "ちなみに、テストデータは`Survived`の列がないので、学習データと連結すると該当部分が空になっています。\n",
    "\n",
    "<br>\n",
    "\n",
    "これを確認したいのですが、そのためには`df.tail()`を使ってあげます。`df.head()`が先頭5行だったのに対して、`df.tail()`は最後の5行を確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最後の5行を確認\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確認してみると、`Survived`の中身が空になっていることが分かりますね。\n",
    "\n",
    "これで学習データとテストデータが、しっかり連結できていることが分かりました。\n",
    "\n",
    "<br>\n",
    "\n",
    "あとは、各データの特徴を確認していきましょう。\n",
    "\n",
    "### データの可視化\n",
    "\n",
    "各カラム(=列)に、どんなデータが入っているのか確認するときは、グラフにしてデータの可視化をしてあげると良いです。これは、自分で見るときもそうだし、相手に見せるときも含めてですね！\n",
    "\n",
    "<br>\n",
    "\n",
    "細かく見ていくと、それだけで3本くらいの動画にできてしまうので、今回は簡単に以下の項目だけデータの可視化をやっていきたいと思います。\n",
    "\n",
    "- 性別 : どんな内訳になっているのか？\n",
    "- チケットのクラス : どんな階級の人が多いのか？\n",
    "\n",
    "なお、性別やチケットのクラスなど、各カテゴリーの数値を確認したいときは、**棒グラフ(barplot)**を使ってあげると良いです。\n",
    "\n",
    "<br>\n",
    "\n",
    "今回は練習のため「性別 : どんな内訳になっているのか？」を確認する3つの方法を紹介していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matplotlibで表示する方法\n",
    "\n",
    "`matplotlib`で男女の内訳を確認するには、まず各性別ごとの人数を集計する必要があります。\n",
    "\n",
    "つまり、①性別ごとにグループ分け(=男性と女性に分ける)して、②各性別ごとにカウントするってことになります。\n",
    "\n",
    "<br>\n",
    "\n",
    "これをコードで書いていきましょう。以下のように書くと、性別ごとの集計をおこなえます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ①性別ごとにグループ分けして、②各性別ごとにカウントする\n",
    "df.groupby('Sex').agg({'Sex': 'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、男性と女性の数を集計できました。\n",
    "\n",
    "この集計結果を、変数`tmp`に入れておきたいと思います。なお、変数に格納するとき、カラム名の`Sex`は`count_sex`とかに変更しておきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 集計結果を変数tmpに格納する\n",
    "tmp = df.groupby('Sex').agg({'Sex': 'count'}).rename(columns={'Sex': 'count_sex'})\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで良いですね。あとは、グラフにするだけです。\n",
    "\n",
    "`matplotlib`で棒グラフを作成するには、`plt.bar(x軸で使う列(=カラム), y軸で使う列(=カラム))`と書いてあげます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グラフの大きさを設定\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 性別の数を確認してみる\n",
    "plt.bar(tmp.index, tmp.count_sex)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、各性別の数を確認できました。グラフにすると、直感的に男性が多いと分かりますね。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandasで表示する\n",
    "\n",
    "なお、今回`matplotlib`を使って書いたコードは、`Pandas`だけでも実現可能です。\n",
    "\n",
    "`Pandas`を使う場合、以下のように`tmp`の後に少し書き足すだけで良いので、簡単なグラフ作成なら、こちらを選択すると良いですね。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandasで棒グラフを作成する\n",
    "tmp.plot(kind='bar', figsize=(10, 6))\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seabornで表示する\n",
    "\n",
    "`matplotlib`や`pandas`でグラフを作成するとき、いったん集計を挟んでから棒グラフを作成しました。\n",
    "\n",
    "でも、これは少し面倒に感じますし、今は「女性・男性」の並び順になっていますが、「男性・女性」の並び順に変更した方が、感覚的に分かりやすい気がします。\n",
    "\n",
    "さらに、それぞれの棒に色付けしてあると、もっと見やすいグラフになりそうです。\n",
    "\n",
    "<br>\n",
    "\n",
    "そんなときは`seaborn`を使ってあげるとお悩み解決できて、`sns.countplot()`を使うことで簡単に男女の人数をカウントした棒グラフを作成できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グラフの大きさを設定\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 性別の数を確認してみる\n",
    "sns.countplot('Sex', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`seaborn`を利用すると、集計するコードを書かなくて良いですし、色付けもよしなにやってくれます。\n",
    "\n",
    "色々な意味でラクできるので、特に棒グラフの作成をするなら`seaborn`を使ってあげると良いですね。\n",
    "\n",
    "<br>\n",
    "\n",
    "もちろん、男性と女性のように2種類を表示するときだけではなく、それ以上の場合でも色付けしてくれます。\n",
    "\n",
    "チケットのクラス(=`Pclass`)でも、`seaborn`を使って可視化してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グラフの大きさを設定\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# seabornで棒グラフを作成する\n",
    "sns.countplot('Pclass',data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、3種類あってもキレイに表示できました。\n",
    "\n",
    "`Pclass`は1番が最も良いので、「やっぱり高級なチケットは、1番お客さんが少ないのかな...」と思いきや、意外にも2番目(=真ん中のクラス)が最も少ないことが分かりましたね。\n",
    "\n",
    "<br>\n",
    "\n",
    "このようにデータを見てあげると色々なことが分かってきます。\n",
    "\n",
    "さらにデータの可視化を深掘ることで、機械学習で使うカラム(=特徴量)を決めていくのですが、今回はこれくらいにして次に進みたいと思います。\n",
    "\n",
    "ただ、機械学習で使うカラムは決めておく必要があるので、今回は「以下4つの特徴量を使う」としましょう。\n",
    "\n",
    "- チケットのクラス\n",
    "- 年齢\n",
    "- 性別\n",
    "- 港"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 欠けているデータはあるのかな？\n",
    "\n",
    "データの可視化と一緒にやるべきことが、欠損値の確認です。\n",
    "\n",
    "欠けているデータがあるなら、それを補ったり削除したりする必要があります。\n",
    "\n",
    "欠損値を補完する理由は、\"基本的に\"欠損値があると機械学習でうまく予測できないからです。\n",
    "\n",
    "*※\"基本的に\"と言っているからには、欠損値があっても機械学習できるアルゴリズムが存在します。ただ、ほとんどの機械学習アルゴリズムで欠損値の補完が必要になります。*\n",
    "\n",
    "<br>\n",
    "\n",
    "欠損値の確認方法はカンタンで、以下のように書くだけでOKです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ内の欠損値を確認する\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記のように書くことで、各項目で欠けているデータを確認できます。\n",
    "\n",
    "例えば`Cabin`=客室番号は、そのほとんどが欠けてしまっていることが分かります。\n",
    "\n",
    "<br>\n",
    "\n",
    "他にも、乗客の年齢(=`Age`)やタイタニック号に乗った港(=`Embarked`)、またチケットの料金(=`Fare`)が欠けていることが分かります。\n",
    "\n",
    "*※`Survived`はテストデータに元から入っていなかったので、気にしなくて大丈夫です!*\n",
    "\n",
    "<br>\n",
    "\n",
    "このように、欠損値の有無とその数を確認することで、欠損値の対処方法を考えていきます。\n",
    "\n",
    "では、どのように欠損値を埋めていくのか。それは、次のステップで紹介していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの加工・前処理・特徴量エンジニアリング\n",
    "\n",
    "データの中身を確認したら、機械学習で使うためのデータに変換していきます。\n",
    "\n",
    "このフェーズを、「データの前処理」とか「データ加工」と言います。\n",
    "\n",
    "また、機械学習で良い結果を出すために、新しいカラムを作成することを特徴量エンジニアリングと言います。機械学習で使うカラム(=特徴量)を作り出す(=エンジニアリング)ってことですね。\n",
    "\n",
    "<br>\n",
    "\n",
    "データの可視化で確認したとおり、タイタニックのデータには欠損値が存在することが分かりました。\n",
    "\n",
    "今回は、欠損値が存在したカラムの中でも、乗客の年齢(=`Age`)と出発した港(=`Embarked`)を使っていきたいです。\n",
    "\n",
    "なので、このステップでは、データ前処理として「欠損値の補完」をやっていきたいと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 年齢とタイタニック号に乗った港の欠損値を埋める\n",
    "\n",
    "そもそも欠損値の対処方法として、補完するだけではなく、削除する方法もあります。\n",
    "\n",
    "例えば、年齢が分からない乗客は使わないってことですね。\n",
    "\n",
    "<br>\n",
    "\n",
    "でも、今回使っているタイタニックデータは、全部で1309件しかありません。\n",
    "\n",
    "元からデータ量が多いわけではないので、年齢が分からない乗客を削除してしまうと、さらにデータ量が減ってしまいます。\n",
    "\n",
    "<br>\n",
    "\n",
    "こういった背景があり、今回は欠損値を「削除」するのではなく「補完」する方法を選びました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embarked\n",
    "\n",
    "まずは、タイタニック号に乗った港の欠損値補完をしていきましょう。\n",
    "\n",
    "そもそも`Embarked`にどれくらいの欠損があったか、再度確認しておきたいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embarkedの欠損値を確認する\n",
    "print('欠損値の数：', df.Embarked.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Embarked`は、2人の乗客だけデータが欠けていることが分かりました。\n",
    "\n",
    "全体から考えて、そこまで欠損値が多いわけではないですね。なので、今回は最も乗客が多かった港で補ってあげたいと思います。\n",
    "\n",
    "<br>\n",
    "\n",
    "タイタニック号に乗った港の数を確認したいのですが、それには`sns.countplot()`を使ってあげると、カンタンに確認できましたね。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グラフの大きさを設定\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 乗船した港を確認する\n",
    "sns.countplot('Embarked', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可視化してみると、ほとんどの乗客が`S`という港から乗ってきたことが分かります。\n",
    "\n",
    "というわけで、欠けている2箇所を`S`で補っていきましょう。欠損値を補完するには、`fillna(\"補完したい値\")`のように書いてあげます。\n",
    "\n",
    "<br>\n",
    "\n",
    "なお、データに変更を加える前に、`df.copy()`を使って元データをコピーしてから欠損値の補完をしていきたいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 元データをコピー\n",
    "df2 = df.copy()\n",
    "\n",
    "# 欠損値の補完\n",
    "df2.Embarked = df2.Embarked.fillna('S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように書くことで、出発した港`Embarked`の補完が完了しました。\n",
    "\n",
    "ちゃんと補完できているか確認するには、以下のように書いてあげれば大丈夫ですね。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乗船した港の欠損値を再度確認する\n",
    "print('欠損値の数：', df2.Embarked.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "しっかり欠損値を補完できていることが分かります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age\n",
    "\n",
    "出発した港と同様に、乗客の年齢も補完していきましょう。\n",
    "\n",
    "まずは、乗客の年齢がどうなっているのか確認していきたいと思います。より具体的にいうと「どの年齢層の乗客が多いのか」を把握しておきたいですね。\n",
    "\n",
    "<br>\n",
    "\n",
    "このように、各層ごとのボリュームを見たい場合には、**ヒストグラム**を書いてあげます。`sns.distplot()`を使うと、カンタンにヒストグラムの描画できます。\n",
    "\n",
    "今回は10代、20代のように、年齢層ごとに確認していきたいので、まずは乗客の年齢における最大値・最小値を確認しておきましょう。\n",
    "\n",
    "<br>\n",
    "\n",
    "最大値・最小値を確認する方法もカンタンで、以下のように書いてあげるだけです！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 年齢の最小値と最大値を確認\n",
    "print('最小値:', df.Age.min())\n",
    "print('最大値:', df.Age.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "タイタニック号の乗客は、上が80歳、下が0歳児までいることが分かります。\n",
    "\n",
    "ということは、乗客の年齢を8コに分割してあげると、各年代ごとのボリュームを確認できますね。\n",
    "\n",
    "<br>\n",
    "\n",
    "分割する数を指定してヒストグラムを描くには、`sns.distplot()`のカッコ内に`bins=8`と書いてあげます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ヒストグラムを作成する\n",
    "sns.distplot(df.Age, kde=False, bins=8)\n",
    "plt.show()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そうすると、各年代ごとのボリュームが可視化できました。\n",
    "\n",
    "作成したヒストグラムを見てみると、20〜30代の人が多いですね。次いで30〜40代が多く、その次に多いのが10〜20代ということが分かります。\n",
    "\n",
    "<br>\n",
    "\n",
    "10〜20代が多いということは「家族連れの乗客が多いのではないか？」といった仮説が立ちますね。\n",
    "\n",
    "そうなると、`SibSp`(=同乗している兄弟/配偶者の数)とか`parch`(=同乗している親/子供の数)も合わせて使うことで、「家族まとめて生存したか否か」といった、新しいカラム(=特徴量)を作成できそうですね。\n",
    "\n",
    "<br>\n",
    "\n",
    "話を戻して、今回は欠損値の補完をやっていきましょう。\n",
    "\n",
    "どの数値で補完していくかですが、このグラフを見ると、年齢の中央値を使ってあげると良さそうです。\n",
    "\n",
    "<br>\n",
    "\n",
    "他にも平均値で補完する方法がありますが、グラフを見ると0〜20歳より30〜80歳が多いので、明らかに平均年齢は高くなりそうです。\n",
    "\n",
    "実際の数値を確認してみましょう。最大値・最小値に習って、以下のように書いてあげます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 年齢の平均値と中央値を確認する\n",
    "print('平均値', df.Age.mean())\n",
    "print('中央値', df.Age.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数値で確認してみても、やはり平均年齢は中央値より高くなっています。\n",
    "\n",
    "現実的に考えると年齢情報がない乗客263人が、みんな28歳ってことは考えにくいですが、今回は練習なので中央値で補完してしまいましょう。\n",
    "\n",
    "<br>\n",
    "\n",
    "欠損値を補完するときは、`Embarked`と同様に、先に`df2.copy()`でデータフレームをコピーしておきます。\n",
    "\n",
    "あとは年齢の中央値で補完するので、先に計算して変数`age_median`に格納しておきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2をコピー\n",
    "df3 = df2.copy()\n",
    "\n",
    "# 年齢の中央値を計算\n",
    "age_median = df3.Age.median()\n",
    "age_median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "あとは、`Embarked`と同様に、`fillna()`を使って欠損値の補完をします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 年齢の欠損値を、計算しておいた中央値で補完する\n",
    "df3.Age = df3.Age.fillna(age_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで欠けていた乗客の年齢情報を補完できました。\n",
    "\n",
    "確認の意味を込めて、年齢の欠損値数を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 年齢の欠損値の数を確認する\n",
    "print('欠損値の数：', df3.Age.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "しっかり補完できました。これで、今回使いたいデータの欠損値補完はできたかと思います。\n",
    "\n",
    "あとは、機械学習で生存したか否かを予測...と言いたいところですが、まだデータ前処理でやるべきことがあります。\n",
    "\n",
    "<br>\n",
    "\n",
    "具体的にいうと、乗船した港や性別のように「中身がカテゴリーになっているデータ」に対して、数値化をしてあげる必要があります。\n",
    "\n",
    "機械学習は結局コンピューターの数値計算をするので、性別に入っている`male`や`female`の状態だと上手く機能しないんですね。\n",
    "\n",
    "*※「上手く機能しない」というか、機械学習する段階でエラーになってしまいますm(_ _)m*\n",
    "\n",
    "<br>\n",
    "\n",
    "というわけで、中身がカテゴリーになっている変数に対して、データの前処理をしていきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カテゴリカル変数の数値変換\n",
    "\n",
    "乗船した港や性別のように、中身がカテゴリーになっているデータを「**カテゴリカル変数**」と言います。\n",
    "\n",
    "ここでは、このカテゴリカル変数を数値に変換していきましょう。\n",
    "\n",
    "<br>\n",
    "\n",
    "カテゴリカル変数の数値変換は、色々な方法があるのですが、今回は以下の2つを使っていきたいと思います。\n",
    "\n",
    "- ワンホットエンコーディング\n",
    "\n",
    "ワンホットエンコーディングは、各カテゴリーに対して別のカラムを準備して、「該当する部分には1, そうではない部分には0」を振り分ける方法です。いわゆるフラグ付ですね。\n",
    "\n",
    "たとえば、S, C, Qというカテゴリーを持つ乗船した港の情報であれば、以下のような変換になります。\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>...</th>\n",
    "        <th>乗船した港</th>\n",
    "        <th>S</th>\n",
    "        <th>C</th>\n",
    "        <th>Q</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>...</td>\n",
    "        <td>S</td>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>...</td>\n",
    "        <td>Q</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>...</td>\n",
    "        <td>S</td>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>...</td>\n",
    "        <td>C</td>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "こうすることで、カテゴリカル変数の数値化が可能になります。\n",
    "\n",
    "<br>\n",
    "\n",
    "- ラベルエンコーディング\n",
    "\n",
    "もう一つの方法が、ラベルエンコーディングです。\n",
    "\n",
    "これは、各カテゴリーを純粋に数値変換する方法です。\n",
    "\n",
    "<br>\n",
    "\n",
    "ワンホット同様に、乗船した港を例に考えてみましょう。S, C, Qというカテゴリーは以下のように変換されます。\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>...</th>\n",
    "        <th>変換前</th>\n",
    "        <th>変換後</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>...</td>\n",
    "        <td>S</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>...</td>\n",
    "        <td>Q</td>\n",
    "        <td>2</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>...</td>\n",
    "        <td>S</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>...</td>\n",
    "        <td>C</td>\n",
    "        <td>1</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "S→0, C→1, Q→2のように、ただ数値に置き換わっただけですね。\n",
    "\n",
    "<br>\n",
    "\n",
    "機械学習する前は、このようにワンホットエンコーディングを使ったり、ラベルエンコーディングを使うことで、カテゴリカル変数を数値に変換してあげましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用するデータの絞り込み\n",
    "\n",
    "カテゴリカル変数を数値に変換する前に、機械学習で使うカラムを絞っておきましょう。\n",
    "\n",
    "毎回大きなデータフレームを扱っていると、なんだかややこしくなってきますので。\n",
    "\n",
    "<br>\n",
    "\n",
    "カラムを選ぶときは、①使わないカラムを削除する方法と②使うカラムだけ指定する方法があります。\n",
    "\n",
    "どちらを選択しても良いですが、今回は学習のために①使わないカラムを削除する方法で進めていきましょう。\n",
    "\n",
    "<br>\n",
    "\n",
    "データフレームからカラムを削除するには、以下のように書いてあげます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 今回使わないカラムを削除する\n",
    "df4 = df3.drop(columns=['Cabin', 'Fare', 'Ticket', 'SibSp', 'Parch', 'Name'])\n",
    "\n",
    "# 先頭5行を確認する\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、データの絞り込みができました。\n",
    "\n",
    "ここまでシンプルにすると、あとは性別(=`Sex`)と乗船した港(=`Embarked`)を数値に変換してあげれば、キレイなデータになりそうですね。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カテゴリカル変数1 : 乗船した港の数値変換\n",
    "\n",
    "まずは乗船した港から数値変換していきましょう。\n",
    "\n",
    "このカラムを数値変換するには、ワンホットエンコーディングを使ってあげます。\n",
    "\n",
    "<br>\n",
    "\n",
    "データフレームの1カラムをワンホットエンコーディングするのは非常に簡単で、`Pandas`の`get_dummies()`を使ってあげるだけでカテゴリカル変数を数値に変換できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# まずはワンホットエンコーディングしてみる\n",
    "pd.get_dummies(df4['Embarked'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_dummies()`を使うことで、先ほど例に出したような数値変換が、全1309行にできました。\n",
    "\n",
    "そうしたら元のデータフレームに連結していきたいので、いったん変数に格納してあげましょう。\n",
    "\n",
    "<br>\n",
    "\n",
    "また、今回の変換で作成されたカラムは、`C`とか`Q`だけだと意味不明です。\n",
    "\n",
    "なので、追加で引数`prefix=\"Embarked\"`を付けてあげることで、乗船した港のワンホットエンコーディングであることを明確にしておきましょう。\n",
    "\n",
    "*※これは機械学習で問題があるというより、レポーティングの問題ですね！突然Cとか出てきても、このデータを知らない人からしたら、まったく持って意味不明です！*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ワンホットの結果を変数tmp_embarkedに格納する\n",
    "tmp_embarked = pd.get_dummies(df4['Embarked'], prefix=\"Embarked\")\n",
    "tmp_embarked.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新たな引数`prefix`を渡すことで、元々のカラム名に各カテゴリーが書かれた形になりました。分かりやすいですね。\n",
    "\n",
    "あとは、元のデータフレームに連結してあげますが、それには`Pandas`の`concat()`を使います。\n",
    "\n",
    "<br>\n",
    "\n",
    "学習データとテストデータのように縦に結合したときも使いましたが、今回のように横に連結するときも使えます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 元のデータフレームにワンホット結果を連結して、変数df5に格納する\n",
    "df5 = pd.concat([df4, tmp_embarked], axis=1).drop(columns=[\"Embarked\"])\n",
    "df5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで乗船した港の情報を数値変換できました。この要領で性別の数値変換もやっていきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カテゴリカル変数の変換2 : 性別\n",
    "\n",
    "性別の変換は、ラベルエンコーディングを使っていきましょう。\n",
    "\n",
    "というのも、性別は2種類のデータしか入っていないので、ワンホットエンコーディングしても、結局2つのカラムは同じ意味になります。\n",
    "\n",
    "具体的に、というところで、実際に性別のカラムでワンホットエンコーディングしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 性別をワンホットエンコーディングする\n",
    "pd.get_dummies(df5.Sex).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ワンホットした結果を見てみると分かりますが、`female`が0のとき`male`が1になり、`female`が1のとき`male`は0になっています。\n",
    "\n",
    "このような場合「男性であるか否か」が分かれば「女性であるか否か」も同時に分かるので、2つのカラムは必要ないんですね。\n",
    "\n",
    "<br>\n",
    "\n",
    "ということで、性別のカラムではラベルエンコーディングを使っていきます。\n",
    "\n",
    "ラベルエンコーディングする方法ですが、今回の場合は以下のように書くことで数値変換できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ラベルエンコーディングした結果を反映する\n",
    "df5['Sex'] = pd.get_dummies(df5.Sex, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフレームの先頭5行を確認する\n",
    "df5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "元のデータフレームと比較すると、男性であれば1、そうでなく女性であれば0ということになります。\n",
    "\n",
    "これでカテゴリカル変数の変換も完了しました。\n",
    "\n",
    "<br>\n",
    "\n",
    "データの前処理ができたので、あとは機械学習にかけて「生存するか否か」を予測してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習用データとテストデータに分割する\n",
    "\n",
    "機械学習を使って生存するか否かを予測するには、「学習で使うデータ」と「生存するか否かを当てたい乗客データ」に分割しておく必要があります。\n",
    "\n",
    "それぞれのデータのことを正確には、以下のように呼びます。\n",
    "\n",
    "- 学習で使うデータ : 学習データ・訓練データ\n",
    "- 生存するか否かを当てたい乗客データ : テストデータ\n",
    "\n",
    "タイタニック号の問題であれば、最初に保存したファイル名が`train.csv`と`test.csv`になっていましたね。\n",
    "\n",
    "<br>\n",
    "\n",
    "今回は、データの可視化や前処理でこれらを連結して処理していたので、もう一度分割してあげましょう。\n",
    "\n",
    "<br>\n",
    "\n",
    "今は難しいことを考えずに、流れをおさえておけば大丈夫です。\n",
    "\n",
    "機械学習をするときは、「学習データとテストデータに分けておくんだ」と覚えておきましょう。\n",
    "\n",
    "<br>\n",
    "\n",
    "それではデータの分割をしていくのですが、そのためには「`Survived`にデータが入っているか否か」で判断すると良いですよね。\n",
    "\n",
    "`Survived`が空になっていることを判定するには、`df5.Servived.isnull()`のように書いてあげます。\n",
    "\n",
    "このことを応用して、学習データとテストデータの分割は、以下のように書けます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データに分割した結果を変数trainに格納する\n",
    "train = df5[~df5.Survived.isnull()]\n",
    "\n",
    "# テストデータに分割した結果を変数trainに格納する\n",
    "test = df5[df5.Survived.isnull()]\n",
    "\n",
    "# テストデータの先頭5行を確認する\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを覗いてみると、しっかり分割できていることが分かりました。\n",
    "\n",
    "テストデータに入っている`Survived`は不要なので、このタイミングで削除しておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survivedを削除\n",
    "test = test.drop(columns=['Survived'])\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "あとは、学習データを「学習に使うカラム(=特徴量)」と「正解(=目的変数)」に分割してあげましょう。\n",
    "\n",
    "特徴量と正解を分割する理由は、機械学習が「特徴量から得た情報と、正解の\"関係性\"を学習して記憶しておく」からです。\n",
    "\n",
    "逆に正解も与えてしまったら、それはただのカンニングになってしまいます。\n",
    "\n",
    "<br>\n",
    "\n",
    "高校の期末試験で最初から問題と答えをもらっていたら、みんな満点になってしまいますよね。それと原理は同じです。\n",
    "\n",
    "それでは面白くないですし、そもそも試験をする意味がなくなってしまいます。機械学習も同じで、答えを知っていたら実施する意味がなくなってしまうので、しっかり特徴量と正解を分割しておきましょう。\n",
    "\n",
    "<br>\n",
    "\n",
    "学習で使うカラムは、元のデータ`train`から`Survived`を削除すれば良いですし、正解は`Survived`だけ取り出せば良いですね！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正解をy_trainに格納する\n",
    "y_train = train.Survived\n",
    "\n",
    "# 特徴量をX_trainに格納する\n",
    "X_train = train.drop(columns=['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで学習データの準備も完了しました。分割した結果を確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量を確認\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正解を確認\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで学習データとテストデータの分割が完了しました。\n",
    "\n",
    "あとは機械学習を使って、タイタニック号の乗客が「生存するか否か」を予測していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 機械学習を使って予測する\n",
    "\n",
    "それでは機械学習を使って、タイタニック号の乗客が「生存するか否か」を予測していきましょう。\n",
    "\n",
    "機械学習は色々なアルゴリズム・モデルがあります。今回は簡単のために、決定木モデルを使って予測していきたいと思います。\n",
    "\n",
    "決定木は、以下の画像のように、条件に応じて分岐をすることで予測する機械学習モデルです。\n",
    "\n",
    "<img src=\"images/01.png\"></img>\n",
    "\n",
    "»参考記事 : https://qiita.com/3000manJPY/items/ef7495960f472ec14377\n",
    "\n",
    "Pythonで決定木を使うには、新しいライブラリ`Scikit-learn`をインポートする必要があります。\n",
    "\n",
    "今回は決定木モデルの作成以外で`Scikit-learn`を使わないので、以下のように書いてライブラリをインポートしていきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで`Scikit-learn`に入っている`tree`をインポートできました。\n",
    "\n",
    "あとは決定木モデルを作成するだけですが、まずはモデル作成の準備として「このモデルを使いますよ〜！」という合図をコードで書いてあげます。\n",
    "\n",
    "それには、以下の一行を書くだけOKです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 決定木モデルの準備\n",
    "model = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tree.DecisionTreeClassifier()`で「決定木の分類木を使う」という意味です。\n",
    "\n",
    "- Decision : 決定\n",
    "- Tree : 木\n",
    "- Classifier : 分類\n",
    "\n",
    "上記のように分割すると分かりやすいですね。\n",
    "\n",
    "<br>\n",
    "\n",
    "「*`Classifier(=分類)`ってなんぞや...*」と思われるかもしれないですね。そもそも機械学習には以下のように「分類」と「回帰」という考え方があります。\n",
    "\n",
    "- 分類 : ラベルを予測すること\n",
    "    - 例：今回のように生存するか否かを当てる\n",
    "- 回帰 : 数値を予測すること\n",
    "    - 例 : 明日の株価(=数値)を当てる\n",
    "\n",
    "<br>\n",
    "\n",
    "なので今回は`tree.DecisionTreeClassifier()`を使っているというわけです。\n",
    "\n",
    "決定木は回帰にも使えるので、その場合には`DecisionTreeRegressor()`を使います。`Regressor`が回帰という意味ですね。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの作成\n",
    "\n",
    "準備した決定木を使って機械学習を開始するには、以下のように`model.fit()`と書いてあげます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 決定木モデルの作成\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この一行を書くだけで決定木を使った機械学習モデルの作成が完了しました。\n",
    "\n",
    "あっという間ですね。「機械学習って、もっと時間がかかるんじゃ...」というイメージを持っていると、わりとビックリするかと思います（笑）\n",
    "\n",
    "今回使っているタイタニックくらいのデータ量であれば、そこまで時間はかからないんですね！\n",
    "\n",
    "<br>\n",
    "\n",
    "また、コードを実行すると色々と出てきます。これは決定木を作ったときのオプションです。\n",
    "\n",
    "<br>\n",
    "\n",
    "決定木のオプションは自分で選択可能で、それには、`tree.DecisionTreeClassifier()`のカッコ内で`tree.DecisionTreeClassifier(max_depth=3)`のように引数で渡してあげます。\n",
    "\n",
    "<br>\n",
    "\n",
    "今回はデータサイエンスの流れをおさえたいので、パラメータの設定はしないでモデル作成しました。\n",
    "\n",
    "本来は、これらのパラメータも色々と設定していくことになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作成したモデルを使って予測する\n",
    "\n",
    "学習データを使った機械学習モデルの作成が完了したので、今度はテストデータを使って予測していきましょう。\n",
    "\n",
    "作成した機械学習モデルを使った予測もカンタンで、以下の一行を書いてあげるだけです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作成した決定木モデルを使った予測をおこなう\n",
    "y_pred = model.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これでテストデータを使った予測が完了しました。中身を確認してみたいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測結果の確認をする\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "予測結果を見てみると、数字の0と1が大量に出てきました。\n",
    "\n",
    "この中にウォーリーはいません。完全に0と1だけ出力されています。\n",
    "\n",
    "<br>\n",
    "\n",
    "この数字は何かというと、今回予測していた「乗客が生存するか否か」です。\n",
    "\n",
    "乗客が生存する場合には1, そうでない場合には0ということになります。\n",
    "\n",
    "<br>\n",
    "\n",
    "少しイメージが付きづらいかもしれないので、元のテストデータに連結してあげましょう。\n",
    "\n",
    "まずは、乗客の数だけ予測結果が存在するか確認してみたいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータと予測結果の大きさを確認する\n",
    "len(test), len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "両方とも418件ということで、各乗客ごとに生存したか否かを予測できたということになります。\n",
    "\n",
    "<br>\n",
    "\n",
    "それでは、予測結果をテストデータに連結していきましょう。\n",
    "\n",
    "予測結果の連結には、テストデータに新しいカラムを追加してあげれば良いですね。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測結果をテストデータに反映する\n",
    "test['Survived'] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで予測結果の連結が完了しました。中身を確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータの先頭5行を確認する\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストデータに予測結果を反映できていますね。\n",
    "\n",
    "予測結果の見方ですが、たとえば892番の人だったら「チケットのクラスが3、年齢が34.5歳、乗ってきた港がQで、決定木モデルは生存できなかったと予測した」ということになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測結果の精度を確認する\n",
    "\n",
    "あとは、決定木モデルの予測結果が本当に正しいのか見ていきましょう。\n",
    "\n",
    "今回使っているKaggleは、指定されているフォーマットでデータを提出すると、モデルの精度を判定できるようになっています。\n",
    "\n",
    "<br>\n",
    "\n",
    "先ほど作成したテストデータ+予測結果のデータから、提出用のシートを作成していきましょう。\n",
    "\n",
    "そのためには、以下のように書いてあげます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提出用のデータマートを作成する\n",
    "pred_df = test[['PassengerId', 'Survived']].set_index('PassengerId')\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで提出用のデータ形式にできました。\n",
    "\n",
    "あとは予測結果`Survived`のカラムが小数表記になっているので、これは整数に変換しておきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測結果を整数に変換する\n",
    "pred_df.Survived = pred_df.Survived.astype(int)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで準備完了です。あとはKaggleにアップするため、CSVに出力してあげましょう。\n",
    "\n",
    "CSVを読み込むとき同様、出力するときも`Pandas`を使っていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSVの作成\n",
    "pred_df.to_csv(\"prediction_data_by_tree_model.csv\", index_label=[\"PassengerId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで提出用のCSVが作成できました。あとはKaggleにアクセスして、今回の予測結果をアップしてみましょう。\n",
    "\n",
    "https://www.kaggle.com/c/titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# まとめ\n",
    "\n",
    "今回の動画シリーズでは、データサイエンスの一連の流れを体験していただきました。\n",
    "\n",
    "本当はもっとデータ分析に時間をかけますし、もっとモデルのチューニングをおこなっていきます。\n",
    "\n",
    "<br>\n",
    "\n",
    "さらに新しい特徴量を作成したり、より適切な欠損値の補完方法を考えたりします。\n",
    "\n",
    "例えば今回、年齢の欠損値補完で中央値を使いました。でも、もっと細かくみていくなら「乗客の名前を元に年齢を補完する」といった施策を考えられますよね。\n",
    "\n",
    "`Name`の中身を見ると、`Mrs.(=結婚している人の敬称)`と`Miss.(=結婚していない人の敬称)`が入っています。\n",
    "\n",
    "敬称に応じた平均年齢はおそらく違ってくるので、そうなると補完に使うべき数字が変わってきますよね。\n",
    "\n",
    "<br>\n",
    "\n",
    "このように、まだまだやるべきことは山積みです。\n",
    "\n",
    "あくまでもこの講義シリーズは、データサイエンスを学び始める「きっかけ作り」にしていただけたらと思います。\n",
    "\n",
    "<br>\n",
    "\n",
    "僕個人としては、データサイエンスひいてはPythonを学び始める人が増えてくれれば何よりです(｀・ω・´)！\n",
    "\n",
    "また、Python・データサイエンス・機械学習など、将来に役立つスキルを無料で学びたいと思ったら、ぜひチャンネル登録していただけると嬉しいです^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "240.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
